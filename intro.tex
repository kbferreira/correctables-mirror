
\section{Introduction}
\label{sec:intro}

Maintaining the performance of high-performance computing~(HPC) applications as
failures become more and more frequent is a major challenge that needs to be
addressed for next-generation extreme-scale systems.  Many recent studies have
demonstrated that hardware failures are expected to become ever more
common~\cite{Bergman08exascalecomputing}.  Increasing the scale of HPC systems
requires the aggregation of larger number of individual components.  More
components means more frequent failures.  Current systems use powerful
error-correcting codes~(ECC), e.g., chipkill-correct, to protect against DRAM
errors.  However, chipkill-correct (and other similar techniques) require the
activation of a large number of memory devices (four times more than
less-protective techniques like single error correct double error
detect~(SECDED))~\cite{Jian13}.  Activating more memory devices requires more
power for each memory access.  However, because of tightening power budgets on
next-generation systems~\cite{Bergman08exascalecomputing}, it is not yet clear
that chipkill-correct will continue to be viable.  Reduced device-feature sizes
also have the potential to result in more frequent failures.  Understanding the
implications of these trends requires that we have detailed knowledge of how
failures affect current leadership-class systems.

Error detection and handling is critical to pinpointing failing components and
taking corrective action in a timely fashion. This error handling is typically a
cooperative activity between the platform hardware, firmware (UEFI or BIOS), and
the host operating system.  Error instances are signaled in host firmware or
the OS directly. The firmware will read the hardware registers and analyze the
component that generated the error in an effort to assess the severity of the
error. Firmware will then create a detailed description of the error and notify
the OS of its occurrence. Host firmware may additionally communicate this error
to baseboard management controllers for system management purposes.  When the
error notification is signaled to the OS, either directly or from host
firmware, the OS typically inspect hardware registers or firmware and initiate
corrective action.  This handling by the OS and/or firmware can perturb
application progress, even in cases where the failure does not initiate a
restart.

Recent works typically focus on uncorrectable errors, those errors that cause
applications to restart.  However, the impacts of the most common error of
large-scale systems, correctable errors, is typically overlooked. An analysis of
failures on recent leadership-class systems show that the correctable failure
rate is a factor \hl{ADD} more frequent than uncorrectable errors.  Correctable
errors are typically handled at a hardware level, and not reflected to the
application.  While the application continues to make progress in the presence
of these correctable errors and does not need to restart,the mechanism to
correct and log these errors have the potential to impact application
performance by delaying application computation. In this work we seek to address
the question of the performance impacts of correctable errors

\input{propagation-fig}
\label{fig:propagation}

In an effort to better understand the impacts from correctable errors, in this
paper we present in-depth analyses that reveal how the interplay between
application and hardware/firmware/OS correctable activities impact performance.
We anticipate that this interplay will become increasingly more important as
node counts and memory volumes increase dramatically on future extreme-scale
systems, further increasing error rates.  Additionally, smaller feature sizes,
manufacturing variances, hardware aging effects, and sub-threshold logic have
the potential to further exacerbate this problem and increase the correctable
and uncorrectable error rates.  More specifically, using a wide array of
applications, we demonstrate that the overheads associated with  correctable
errors can have a significant impact on overall application performance.  These
local correctable errors introduce overheads that can amplified or absorbed
globally by an application depending on an application's communication
activities.  We use profiles of these communication activities as well as
microbenchmark studies to more precisely attribute each application's
performance to its communication operations.

The possibly of correctable error activities inducing delays amongst application
processes, including those that do not directly communicate, is analogous to the
manner in which operating system noise (or \emph{jitter}) can affect HPC
applications~\cite{Hoefler:2010:Characterizing, Ferreira:08:characterizing}.
\Cref{fig:propagation} illustrates this phenomenon.  \Cref{subfig:no_ckpt} shows
a simple application running across three processes ($p_0$, $p_1$, and $p_2$)
with no delays due to CE.  These three processes exchange two messages,
$\mathbf{m}_1$ and $\mathbf{m}_2$.  We assume here that these messages represent
strict dependencies: any delay in the arrival of a message requires the
recipient to stall until the message is received. \Cref{subfig:uncoord_ckpt}
illustrates the potential impacts due to local CE mitigation activity.  If $p_0$
receives a CE at the instant before it would have otherwise sent $\mathbf{m}_1$,
then $p_1$ is forced to wait (the waiting period is shown in grey) until the
message arrives.  If $p_1$ subsequently receives a CE before sending
$\mathbf{m}_2$, then $p_2$ is forced to wait.  Part of the time that $p_2$
spends waiting is due to a delay that was originated by $p_0$, which it does not
communicate with.  The key point is that delays due to correctable error
activities can propagate based on communication dependencies in the application.

Based on the studies presented in this paper, we make the following
contributions:

\begin{itemize}

\item we demonstrate the impacts of the hardware/firmware/OS activities
        associated with correctable errors, and how those activities can changes
        based on platform configuration \S{?};

\item we analyze the potential impacts of this correctable errors on application
        performance and how these impacts many change with increased errors rate
        \S\S{?};

\item we show how an application's scale and communication pattern can dictate
        whether local overheads due to correctables are amplifies or absorbed by
        other processes \S\S{?}; and

\item we show the interplay between the correctable error rate and the duration
        of each mitigation activity, providing prescriptive advice on how best
        to reduce overheads due to these common errors on leadership-class platforms
        \S{?}.

\end{itemize}

Overall, this work provides critical analysis and insight into the overheads of
common correctable errors and provides practical advice to users and systems
administrators in an effort to fine-tune performance to application and system
characteristics.


\section{Related Work}
\label{sec:related}


\kbf{This first paragraph is nearly identical to what is in the parallel
submission}

Efforts to characterize the frequency and type of correctable and uncorrectable
failures on large-scale HPC and cloud systems have been ongoing for over a
decade now.  Schroeder and Gibson studied failures in supercomputer systems at
LANL~\cite{Schroeder:2006:Large-scale}.  Schroeder {\it et al.}~conducted a
large-scale field study using Google's server fleet~\cite{Schroeder09}.  Li {\it
et al.}~studied memory errors on three different data sets, including a server
farm of an Internet service provider~\cite{Li07}.  In 2010, Li {\it et
al.}~published an expanded study of memory errors at an Internet server farm and
other data center sources~\cite{Li10}.  Hwang {\it et al.}~published an expanded
study on Google's server fleet, as well as two IBM Blue Gene
clusters~\cite{Hwang12}.  Sridharan and Liberty presented a study of DRAM
failures in a high-performance computing system~\cite{Sridharan12}.  El-Sayed
{\it et al.}~studied temperature effects of DRAM in data center
environments~\cite{Elsayed12}.  Similarly, Siddiqua {\it et al.}~studied DRAM
failures from client and server systems~\cite{Siddiqua13}.  Sridharan {\it et
al.}~studied DRAM and SRAM faults, with a focus on positional and vendor
effects~\cite{Sridharan13}.  Di Martino {\it et al.}~studied failures in Blue
Waters, an HPC system at the University of Illinois,
Urbana-Champaign~\cite{bluewaters}.  Additionally, Bautista-Gomez {\it et
al.}~\cite{Bautista-Gomez:2016:Unprotected} presented a study of DRAM memory
errors on a large-scale system in the explicit absence of an ECC in an effort
understand the behavior of raw memory failures. \kbf{ADD Christian's SC17 paper}.
Finally, \kbf{ADD our SC18 paper}

Our study has origins in previous works that characterizes application behavior
in the presence of OS
noise~\cite{Ferreira:2008:Characterizing,Hoefler:2010:Characterizing}.
Collectively, this research shows that the pattern of OS noise events determines
the impact on application performance and the benefits of coordination.
Moreover, it shows that the duration of an OS noise event can significantly
slowdown application performance.  Additionally, Delgado and
Karavanic~\cite{Delgado:2013:SMM} examined the impact of system management mode
(SMM) interrupts on network and IO workloads.  Similarly, Macarenco {\it et
al.}~\cite{Macarenco:2016:Effects} examine the impact of SMM mode for small
scale NAS parallel benchmark runs and the UnixBench benchmark.


Most closely related, Gottscho {\it et al.}~\cite{Gottscho:2017:Measuring}
examine the single-machine performance impacts of correctable DRAM errors for
web search and SPEC CPU2006 benchmarks.  In this this work the authors
demonstrate that an "avalanche" of these correctable errors can significantly
impact benchmark performance.  Finally, using a proprietary hardware and
software tool, the authors also characterize the impacts of DRAM correctable
errors for Windows Server 2012 at a hardware, firmware, and OS level.


\kbf{Strengthen this paragraph on why we are novel and awesome}

Our work distinguishes itself from these existing studies in server al important
ways.  First, to the best of our knowledge this is the first study to examine
the HPC performance impacts of \emph{correctable} DRAM errors.  Much of the existing
work is focused on either characterizing failures or examining the application
performance implication of DUEs. In addition, unlike previous work, we attempt a
principled analysis of the correctable error rate increases likely with future
systems as well as the importance of reducing the duration of each memory error
event.  Finally, we examine how an applications communication dependencies,
particularly its collective communication, influence the impacts from correctable
errors \kbf{ADD brief application analysis?}.

\kbf{Anything more to add here?}
